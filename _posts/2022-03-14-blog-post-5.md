---
layout: post
title: Blog Post 5 - Image Classification
---

This blog post is to build a machine learning model to distinguish images of cats and dogs. This is done by apply layers of programs in sequence. We will start from a baseline model and make step-by-step modifications to finally achieve a 95% accuracy! <br>
<br>

### **§1. Load packages and obtain data**
First, let's import all necessary packages:


```python
import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import utils 
import matplotlib.pyplot as plt
import random
from tensorflow.keras import datasets, layers, models
```

and import all the images and labels from the online data base with the settings for later training:


```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```


```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

Before we start to build and train our model, let's write a function to visualize the images of cats and dogs: 


```python
def visualize():
  # Get 32 images from the train_data set and normalize the numpy matrix
  (train_images, train_labels) = list(train_dataset.take(1))[0]
  class_names = ["cat","dog"]
  train_images = train_images/255.0
  train_labels = train_labels.numpy()

  # Separating cat and dog images
  cats = [train_images[i] for i in range(32) if train_labels[i] == 0]
  dogs = [train_images[i] for i in range(32) if train_labels[i] == 1]

  # Ploting the images
  plt.figure(figsize=(10,10))

  for i in range(6):
      plt.subplot(2,3,i+1)
      # take away ticks and label
      plt.xticks([])
      plt.yticks([])
      plt.grid(False)
      if i<3:
        plt.imshow(random.choice(cats))
        plt.xlabel(class_names[0])
      else:
        plt.imshow(random.choice(dogs))
        plt.xlabel(class_names[1])
  plt.show()
```

This `visualize()` function retunrs a row of 3 cat images followed by a row of 3 dog images:


```python
visualize()
```

![output_8_0.png](/images/output_8_0.png)
    


By running the following code a few times, we can see the baseline model (i.e. always guesses the most frequent label) has an accuracy between 50% and 60%.


```python
# baseline model
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
j = 0
for i in range(100):
   j += next(labels_iterator)
j
```

### **§2. First Model**

The first model mainly utilizes kernels with the `Conv2D`, `MaxPooling2D`, `Flatten`, `Dense` and `Dropout` layers, is built with the `models.Sequential` funcations:


```python
model1 = models.Sequential([
layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(2)
])
```

To optimize the runtime, we need to do `model1.complie` and use the Adam optimizer:


```python
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
```

Now, we are ready to train and validate our first model:


```python
history = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

We can now plot the training and validation accuracy against the the number of epoch:


```python
def history_plot():
    plt.plot(history.history["accuracy"], label = "training")
    plt.plot(history.history["val_accuracy"], label = "validation")
    plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
    plt.legend()
    
history_plot()
```

![model1.png](/images/model1.png)

*   **The validation accuracy of my model stablized between 55% and 58% during training.** 
*   Comparing to baseline model, which has an accuracy of roughly 50%, `model1` made some improvement.
*   However, the training accuracy stablized between 95% and 98%, which is much higher than the validation accuracy, suggesting overfitting in `model1`.

<br>


### **§3. Model with Data Augmentation**
The second model features data augmentation layers that randomly flips and rotates the images by using `RandomFlip` and `RandomRotate`. The `RandomRotate`takes an argument `x` between 0 and 2 for `x`*Pi() rotaiton. The effects are demonstrated in the images below:


```python
RandomFlip= tf.keras.Sequential([layers.RandomFlip()])

for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    if i ==0:
      plt.imshow(first_image / 255)
      plt.xticks([])
      plt.yticks([])
      plt.grid(False)
      plt.xlabel("original")

    else:
      augmented_image = RandomFlip(tf.expand_dims(first_image, 0))
      plt.imshow(augmented_image[0] / 255)
      plt.axis('off')
```

![output_22_0.png](/images/output_22_0.png)
    



```python
RandomRotation = tf.keras.Sequential([layers.RandomRotation(random.uniform(0,2))])

for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    if i ==0:
      plt.imshow(first_image / 255)
      plt.xticks([])
      plt.yticks([])
      plt.grid(False)
      plt.xlabel("original")

    else:
      augmented_image = RandomRotation(tf.expand_dims(first_image, 0))
      plt.imshow(augmented_image[0] / 255)
      plt.axis('off')
```

![output_23_0.png](/images/output_23_0.png)
    


We can insert the data augmentation layers before the layers in `model1` to create `model2`, and plot the training and validation accuracy:


```python
data_agumentation = models.Sequential([RandomFlip, RandomRotation])

model2 = models.Sequential([
    data_agumentation,
    model1
])


model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


history = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data = validation_dataset)

history_plot()
```

![output_25_1.png](/images/output_25_1.png)
    


*   **The validation accuracy of my model stablized between 59 and 62% during training.** 
*   Comparing to baseline `model1`, `model2`performs slightly better.
*   The training accuracy is similar to the validation accuracy, suggesting that there is no overfitting in `model2`.

<br>


### **§4. Data Preprocessing**

The next improvement we can make is to scale the 255 values to values between 0 and 1 or between 1 and -1. The mathematically identical scenarios would allow less effort to be spent on adjusting the weights of the data. The `preprocessor` layer is created by the following code:


```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```

We can then insert this `preprocessor` layer before all layers in `model2`, creating `model3`:


```python
model3 = models.Sequential([
    preprocessor, 
    model2
])


model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


history = model3.fit(train_dataset, 
                     epochs=20, 
                     validation_data = validation_dataset)

history_plot()
```
    
![output_31_1.png](/images/output_31_1.png)
    


*   **The validation accuracy of my model stablized between 70 and 75% during training.** 
*   Comparing to baseline `model1`, `model3` performs significantly better.
*   The training accuracy is similar to the validation accuracy, suggesting that there is no overfitting in `model3`.

<br>

### **§5. Transfer Learning**

Now we can use some borrowed knowledge. We can use a pre-existing image recognition model as a layer to our `model4`. The following code imports `MobileNetV2` as the `base_model_layer`:


```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

`model4` is built by the following layers:`prepreprocessor` (Part 4), `data_agumentation` (Part 3), `base_model_layer` (just imported above), `GlobalMaxPooling2D`, `Dropout`and finally `Dense(2)`.<br> As shwon in the `model4.summary()`, the `base_model_layer`contains 2,257,984 programs, eliminating the need for adding many more layers.




```python
model4 = models.Sequential([
    preprocessor,
    data_agumentation,
    base_model_layer,
    layers.GlobalMaxPooling2D(),
    layers.Dropout(0.2),
    layers.Dense(2)
])

model4.build(input_shape=(None, 160, 160, 3))
model4.summary()
```

    Model: "sequential_11"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     sequential_10 (Sequential)  (None, 160, 160, 3)       0         
                                                                     
     model_2 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                     
     global_max_pooling2d_3 (Glo  (None, 1280)             0         
     balMaxPooling2D)                                                
                                                                     
     dropout_5 (Dropout)         (None, 1280)              0         
                                                                     
     dense_8 (Dense)             (None, 2)                 2562      
                                                                     
    =================================================================
    Total params: 2,260,546
    Trainable params: 2,562
    Non-trainable params: 2,257,984
    _________________________________________________________________
    

We can plot the accuracy again and see how much improvement can be made:


```python
model4.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


history4 = model4.fit(train_dataset, 
                     epochs=20, 
                     validation_data = validation_dataset)

history_plot()
```   
![output_39_1.png](/images/output_39_1.png)
    


*   **The validation accuracy of my model stablized between 95 and 98% during training.** 
*   Comparing to baseline `model1`, `model4`almost doubled the accuracy.
*   The training accuracy is similar to the validation accuracy, suggesting that there is no overfitting in `model4`. <br> The validation accuracy is consistantly higher than the training accuracy and the model has generalized fine.

<br>

### **§6. Score on Test Data**

Finally, we can test our best-performing model `model4` with the `test_dataset`! The model can achieve a 97% accuracy!!


```python
loss, accuracy = model4.evaluate(test_dataset)
print('Test accuracy :', accuracy)
```

    6/6 [==============================] - 2s 337ms/step - loss: 0.1164 - accuracy: 0.9688
    Test accuracy : 0.96875
    
