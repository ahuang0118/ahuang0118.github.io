---
layout: post
title: Blog Post 4: Spectral Clustering
---

# Blog Post 4: Spectral Clustering

This blog post is to build a simple version of the *spectral clustering* algorithm for clustering data points.
>**Please note that I kept all the maths from the prompt and put my programming comments in note boxes like this.**

### Notation
In all the math below: 
- Boldface capital letters like $\mathbf{A}$ refer to matrices (2d arrays of numbers). 
- Boldface lowercase letters like $\mathbf{v}$ refer to vectors (1d arrays of numbers). 
- $\mathbf{A}\mathbf{B}$ refers to a matrix-matrix product (`A@B`). $\mathbf{A}\mathbf{v}$ refers to a matrix-vector product (`A@v`). 

### Introduction
Let's import all the modules that will be needed:


```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```

*Clustering* seperates data set into the different natural "blobs". K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these: 


```python
# Creating blob data set
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)

# Clustering using K-mean
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x1d3ccc4bdc8>




    
![png](/images/output_3_1.png)
    


The Euclidean coordinates of the data points are contained in the matrix `X`, while the labels of each point are contained in `y`. However, this becomes harder when the blobs are not circular:


```python
# Creating moon data set 
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)

# Clustering using K-mean
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x1d3cedeb5c8>




    
![png](/images/output_5_1.png)
    


Since K-means is, by design, looking for circular clusters, we need to do spectral clustering to efficiently separate non-circular clusters.

### Part A - Similarity Matrix

The first step is to construct the *similarity matrix* $\mathbf{A}$. $\mathbf{A}$ should be a matrix (2d `np.ndarray`) with shape `(n, n)` (recall that `n` is the number of data points). 

When constructing the similarity matrix, we use a parameter `epsilon`. Entry `A[i,j]` should be equal to `1` if `X[i]` (the coordinates of data point `i`) is within distance `epsilon` of `X[j]` (the coordinates of data point `j`), and `0` otherwise. The diagonal entries `A[i,i]` should all be equal to zero.

For this part, we set `epsilon = 0.4`. 

>The distance between points `i` and `j` in the dataset can be easily found by the `pairwise_distances` function  in `sklearn.metrics`. This returns a `(n, n)` matrix with all the pairwise distances. Then, we can use `np.fill_diagonal` to set all diagonal entries to 0:


```python
n = 200
epsilon = 0.4

from sklearn.metrics import pairwise_distances
X_dis = pairwise_distances(X)
A = (X_dis < epsilon) + 0 # converts boolean matrix to 0 and 1
np.fill_diagonal(A,0) # set diagonal entries to 0
A
```




    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 1, 0],
           ...,
           [0, 0, 0, ..., 0, 1, 1],
           [0, 0, 1, ..., 1, 0, 1],
           [0, 0, 0, ..., 1, 1, 0]])



### Part B - Computing binary norm cut objective

The matrix `A` now contains information about which points are near (within distance `epsilon`) which other points. We now pose the task of clustering the data points in `X` as the task of partitioning the rows and columns of `A`. 

Let $d_i = \sum_{j = 1}^n a_{ij}$ be the $i$ th row-sum of $\mathbf{A}$, which is also called the *degree* of $i$. Let $C_0$ and $C_1$ be two clusters of the data points. We assume that every data point is in either $C_0$ or $C_1$. The cluster membership as being specified by `y`. We think of `y[i]` as being the label of point `i`. So, if `y[i] = 1`, then point `i` (and therefore row $i$ of $\mathbf{A}$) is an element of cluster $C_1$.  

The *binary norm cut objective* of a matrix $\mathbf{A}$ is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, 
- $\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} A_{ij}$ is the *cut* of the clusters $C_0$ and $C_1$. 
- $\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$, where $d_i = \sum_{j = 1}^n A_{ij}$ is the *degree* of row $i$ (the total number of all other rows related to row $i$ through $A$). The *volume* of cluster $C_0$ is a measure of the size of the cluster. 

A pair of clusters $C_0$ and $C_1$ is considered to be a "good" partition of the data when $N_{\mathbf{A}}(C_0, C_1)$ is small. To see why, let's look at each of the two factors in this objective function separately. 


#### B.1 The Cut Term

First, the cut term $\mathbf{cut}(C_0, C_1)$ is the number of nonzero entries in $\mathbf{A}$ that relate points in cluster $C_0$ to points in cluster $C_1$. Saying that this term should be small is the same as saying that points in $C_0$ shouldn't usually be very close to points in $C_1$. 

>Let's write a function called `cut(A,y)` to compute the cut term, by summing up the entries `A[i,j]` for each pair of points `(i,j)` in different clusters. This can be done by running a double for-loop through `y`. For the `i` and `j` points that have different values of `y`, we include the $A_{ij}$ in the sum. 
**Note that by running this double for loop we will include both $A_{ij}$ and $A_{ji}$, so we need to divide the sum by 2.**



```python
def cut(A, y):
    C = 0
    for i in range(n):
        for j in range(n):
            if y[i] != y[j]:
                C += A[i,j]

    return C/2
```

We can test this function by using the true clusters `y` and a randomly generate vector of length `n`, with each label equal to either 0 or 1 (using `np.random.randint`). As shown below, the cut term is much smaller for the the true clusters `y`:


```python
C = cut(A,y)
C
```




    13.0




```python
y_random = np.random.randint(0,2,size=(n,))
C_random = cut(A, y_random)
C_random
```




    1150.0



#### B.2 The Volume Term 

Now take a look at the second factor in the norm cut objective. This is the *volume term*. As mentioned above, the *volume* of cluster $C_0$ is a measure of how "big" cluster $C_0$ is. If we choose cluster $C_0$ to be small, then $\mathbf{vol}(C_0)$ will be small and $\frac{1}{\mathbf{vol}(C_0)}$ will be large, leading to an undesirable higher objective value. 

Synthesizing, the binary normcut objective asks us to find clusters $C_0$ and $C_1$ such that:

1. There are relatively few entries of $\mathbf{A}$ that join $C_0$ and $C_1$. 
2. Neither $C_0$ and $C_1$ are too small. 

> Let's a write a function called `vols(A,y)` which computes the volumes of $C_0$ and $C_1$, returning `v0` and `v1` as a tuple, representing the volumes of cluster `0` and cluster `1` respectively. The degree vector $\mathbf{d}$ can be easily computed by `A.sum(axis=1)`. The dot product of `d` and `y` is the naturally the volume of cluster `1`. `V_0` can then be computed by substraing `V_1` from the sum of all entries of $\mathbf{A}$ or $\mathbf{d}$:



```python
def vols(A, y):
    d = A.sum(axis=1)
    v1 = d@y
    v0 = A.sum() - v1
    return v0, v1
```

>Then, we can write a function called `normcut(A,y)` which uses `cut(A,y)` and `vols(A,y)` to compute the binary normalized cut objective of a matrix `A` with clustering vector `y`, based on the equation above:  


```python
def normcut(A,y):
    c = cut(A, y)
    v0, v1 = vols(A, y)
    return c*(1/v0 + 1/v1)
```

Now, we can compare the `normcut` objective using both the true labels `y` and the randomly generated fake labels above. Indeed, the `normcut` objective of the true `y` is significantly lower than that of the `y_random`:


```python
print(normcut(A,y))
print(normcut(A,y_random))
```

    0.011518412331615225
    1.0240023597759158
    

### Part C - vector $\mathbf{z}$

We have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in $A$ and (b) not too small. One approach to clustering is to try to find a cluster vector `y` such that `normcut(A,y)` is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick! 

Here's the trick: define a new vector $\mathbf{z} \in \mathbb{R}^n$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


Note that the signs of  the elements of $\mathbf{z}$ contain all the information from $\mathbf{y}$: if $i$ is in cluster $C_0$, then $y_i = 0$ and $z_i > 0$. 

Next, if you like linear algebra, you can show that 

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $\mathbf{D}$ is the diagonal matrix with nonzero entries $D_{ii} = d_i$, and  where $d_i = \sum_{j = 1}^n a_i$ is the degree (row-sum) from before.  

> Let's write a function called `transform(A,y)` to compute the appropriate $\mathbf{z}$ vector given `A` and `y`, using the formula above. <br>
>For $z_i $ when $y_i = 1$, $z_i = y_i / {\mathbf{vol}(C_1)} $; <br>for $z_i $ when $y_i = 0$, $z_i = -(y_i - 1) / {\mathbf{vol}(C_0)}$:




```python
def transform(A,y):
    v0, v1 = vols(A, y)
    z = -y / v1 - (y-1) / v0
    return z 
```

> Now we can check the equation above that relates the matrix product to the normcut objective, by computing each side separately and checking that they are equal.  $\mathbf{z}^T\mathbf{D}\mathbf{z}$ can be computed as `z@D@z`, provided that you have constructed these objects correctly. `np.isclose` is used since the calculations are not excat:



```python
D = np.zeros((n,n))
np.fill_diagonal(D,A.sum(axis=1))
z = transform(A,y)
np.isclose(normcut(A,y),(z@(D-A)@z)/(z@D@z))
```




    True



> We can also check the identity $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$, where $\mathbb{1}$ is the vector of `n` ones (i.e. `np.ones(n)`). This identity effectively says that $\mathbf{z}$ should contain roughly as many positive as negative entries. 



```python
one = np.ones(n)
z@D@one
```




    0.0



### Part D - compute norm cut using vector $\mathbf{z}$
In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$. It's actually possible to bake this condition into the optimization, by substituting for $\mathbf{z}$ the orthogonal complement of $\mathbf{z}$ relative to $\mathbf{D}\mathbf{1}$. In the code below, an `orth_obj` function is defined to handle this: 


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

>Now, we can use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $\mathbf{z}$. This requires an intial guess of $\mathbf{z}$, which can be generated as random numbers between 1 and -1. `z_min` is the `x` attribute of the output:


```python
from scipy.optimize import minimize
z_0 = np.random.randint(-1,2,size=(200,))
z_min = minimize(orth_obj, z_0).x
```

**Note**: there's a cheat going on here! We originally specified that the entries of $\mathbf{z}$ should take only one of two values (back in Part C), whereas now we're allowing the entries to have *any* value! This means that we are no longer exactly optimizing the normcut objective, but rather an approximation. This cheat is so common that deserves a name: it is called the *continuous relaxation* of the normcut problem. 

### Part E - testing `z_min`

Recall that, by design, only the sign of `z_min[i]` actually contains information about the cluster label of data point `i`. Plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. 

> Now, we can check whether we have made any improvements to the clustering. As seen below, we did make huge progress! Most of the points are sorted into the correct clusters except for a few anomalies:


```python
plt.scatter(X[:,0], X[:,1], c =z_min>=0)
```




    <matplotlib.collections.PathCollection at 0x1d3d5c5f1c8>




    
![png]/images/output_32_1.png)
    


### Part F - $Laplacian$ matrix

Explicitly optimizing the orthogonal objective is  *way* too slow to be practical. If spectral clustering required that we do this each time, no one would use it. 

The reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called *spectral* clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices. 

Recall that what we would like to do is minimize the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to $\mathbf{z}$, subject to the condition $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$. 

The Rayleigh-Ritz Theorem states that the minimizing $\mathbf{z}$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

Why is this helpful? Well, $\mathbb{1}$ is actually the eigenvector with smallest eigenvalue of the matrix $\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$. 

So, the vector $\mathbf{z}$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

> We can construct the matrix $\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$, which is often called the (normalized) *Laplacian* matrix of the similarity matrix $\mathbf{A}$. Then, we need to find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`. We can find all eigenvalues and corresponding eigenvectors of a matrix using `np.linalg.eig()`. We can find the index of the second smallest eigenvalue and `z_eig` by using `np.argpartition`:


```python
# compute D_inv and L
D_inv = np.linalg.inv(D)
L = D_inv@(D - A)
# Find all eigenvalues and eigenvectors using L
all_lam, all_z_eig = np.linalg.eig(L)
# Find the index of the second smallest eigenvalue
z_eig_index = np.argpartition(all_lam, 2)[1]
# Find z_eig
z_eig = all_z_eig[:,z_eig_index]
# Test the result with Rayleigh-Ritz Theorem
np.allclose(L@z_eig, all_lam[z_eig_index]*z_eig)
```




    True



> As we can see from the graph below, this method is as good as explicitly optimizing the orthogonal objective:


```python
plt.scatter(X[:,0], X[:,1], c =z_eig>=0)
```




    <matplotlib.collections.PathCollection at 0x1d3d9ae6448>




    
![png](/images/output_36_1.png)
    


In fact, `z_eig` should be proportional to `z_min`, although this won't be exact because minimization has limited precision by default. 

### Part G - Synthesize `spectral_clustering(X, epsilon)`

> Now, we are going to write a very compact function called `spectral_clustering(X, epsilon)` that does the following:
>1. Construct the similarity matrix. 
>2. Construct the Laplacian matrix. 
>3. Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix. 
>4. Return binary labels based on this eigenvector. 


```python
def spectral_clustering(X,epsilon):
    # Construct the similarity matrix
    X_dis = pairwise_distances(X)
    A = (X_dis < epsilon) + 0
    np.fill_diagonal(A,0)
    # Construct the Laplacian matrix 
    D = np.zeros((len(X),len(X)))
    np.fill_diagonal(D,A.sum(axis=1))
    L = (np.linalg.inv(D))@(D - A)
    # Find eigenvector of second smallest eigenvalue
    all_lam, all_z_eig = np.linalg.eig(L)
    z_eig = all_z_eig[:,np.argpartition(all_lam, 2)[1]]
    # return binary labels
    return (z_eig>=0) 
```

> This function works well with the dataset:


```python
plt.scatter(X[:,0], X[:,1], c =spectral_clustering(X,epsilon))
```




    <matplotlib.collections.PathCollection at 0x1d3d5905388>




    
![png](/images/output_41_1.png)
    


### Part H - Experimenting with `noise`

> We can now experiement the synthesized `spectral_clustering(X, epsilon)` with different `noise`. Here I chose 0.1, 0.2 and 0.3.



```python
# noise = 0.1
X_H, y_H = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.1, random_state=None) 
plt.scatter(X_H[:,0], X_H[:,1], c = y_H)
```




    <matplotlib.collections.PathCollection at 0x1d3d83db1c8>




    
![png](/images/output_43_1.png)
    



```python
# noise = 0.1
plt.scatter(X_H[:,0], X_H[:,1], c = spectral_clustering(X_H,epsilon))
```




    <matplotlib.collections.PathCollection at 0x1d3d8451508>




    
![png](/images/output_44_1.png)
    


> The function works well when `noise` is increased to 0.1. Now, let's try 0.2. As seen below,the boundary between the two clusters becomes blurry and the function doesn't work so effectively:


```python
# noise = 0.2
X_H, y_H = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.2, random_state=None) 
plt.scatter(X_H[:,0], X_H[:,1], c = y_H)
```




    <matplotlib.collections.PathCollection at 0x1d3d853fa48>




    
![png](/images/output_46_1.png)
    



```python
# noise = 0.2 
plt.scatter(X_H[:,0], X_H[:,1], c = spectral_clustering(X_H,epsilon))
```




    <matplotlib.collections.PathCollection at 0x1d3d85bed88>




    
![png](/images/output_47_1.png)
    


> If we increase `noise` to 0.3, we will see `LinAlgError: Singular matrix` with this the synthesized `spectral_clustering(X, epsilon)` function because $\mathbf{D}$ becomes singular. This can be overcome by increasing `epsilon`. However, good separation still can't be acheived:


```python
# noise = 0.3
X_H, y_H = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.3, random_state=None) 
```




    <matplotlib.collections.PathCollection at 0x1d3d87b53c8>




    
![png](/images/utput_49_1.png)
    



```python
# noise = 0.3 
plt.scatter(X_H[:,0], X_H[:,1], c = spectral_clustering(X_H,epsilon=0.6))
```




    <matplotlib.collections.PathCollection at 0x1d3d9797bc8>




    
![png](/images/output_50_1.png)
    


### Part I - testing with the bull's eye

Now we can try spectral clustering function on another data set -- the bull's eye! 


```python
X_I, y_I = datasets.make_circles(n_samples=1000, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x1d3db48e7c8>




    
![png](/images/output_52_1.png)
    


There are two concentric circles. As before k-means will not do well here at all. 


```python
km = KMeans(n_clusters = 2)
km.fit(X_I)
plt.scatter(X_I[:,0], X_I[:,1], c = km.predict(X_I))
```




    <matplotlib.collections.PathCollection at 0x1d3db4f82c8>




    
![png](/images/output_54_1.png)
    


> Now we can experiement with values of `epsilon` between `0` and `1.0`. <br>For `epsilon` between `0.2` and `0.5` (and oddly not `0.3`), the function is able to correctly separate the two rings; <br>For `epsilon` below `0.2`, there is no separation and is likely to have `LinAlgError` <br>;For `epsilon` above `0.3`, the result is similar to that of `KMeans`


```python
plt.scatter(X_I[:,0], X_I[:,1], c = spectral_clustering(X_I,epsilon=0.35)) # works for 0.4 to 0.5
```




    <matplotlib.collections.PathCollection at 0x1d3d9304308>




    
![png](/images/output_56_1.png)
    

