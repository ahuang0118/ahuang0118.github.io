---
layout: post
title: Blog Post 6 - Fake News Detector
---


This blog post is to build a fake news detector using TensorFlow word embedding and visualize the embedding using Plotly. First, we need to import all the necessary packages:


```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup
from tensorflow import keras

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# for embedding viz
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
import matplotlib.pyplot as plt
```
<br>
### Acquiring training data and build a dataset

The training data can be acquired from the URL below and read as pandas dataframe `train_df`:


```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
train_df = pd.read_csv(train_url)
```

The dataframe contains the title and text of news entries and binary classfication of whether they are fake. <br> <br>
Before we train the model, we need to remove all the stopwords from the original. We can find the stopwords using the `stopwords.words` attribute in `nltk`. The removal can be done by applying `lambda` functions to the original columns of the dataframe. The `pd` dataframe then needs to be converted to `tf` dataset. In order to reduce the processing time later, we can use `batch` to train the models in chucks of data rather than individual rows. We can create a `make_dataset(df)` function to carry out the above operations:





```python
def make_dataset(df):
  from nltk.corpus import stopwords
  stopwords = stopwords.words('english')
  # remove stop words from initial dataset
  df["title_nsw"] = df["title"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))
  df["text_nsw"] = df["text"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))
  # convert pd dataframe to tf dataset
  my_data_set = tf.data.Dataset.from_tensor_slices(
    ({"title" : df[["title_nsw"]], 
            "text" : df[["text_nsw"]]}, 
        { "fake" : df[["fake"]]})
    ) 
  # batch dataset to reduce runtime
  my_data_set = my_data_set.batch(100)
  return my_data_set
```

Next, we are going to perform a train/validation split: 80% for training and 20% for validation:


```python
data = make_dataset(train_df)
data = data.shuffle(buffer_size = len(data))

train_size = int(0.8*len(data))
val_size   = int(0.2*len(data))

train = data.take(train_size)
val   = data.skip(train_size).take(val_size)
```

Prior to training our models, we can take a look at the base rate, which turns out to be 52%:


```python
# base rate
fake_rate = [sum(b["fake"]) for a,b in train]
float(sum(fake_rate) / len(train))
```




    52.272222222222226


<br>
### Model 1 - title only
We will train `model`on using the "text" data only. We need pre-process and vectorize the string data before building the model. The `standardization` funcation removes all the punctuation in the initial data:


```python
# Preprocessing data
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 
```

`vectorize_layer` converts input strings to numbers based on their frequency appeared in all title entries. This is done by `vectorize_layer.adapt`. We can specify the input data to be title only using `keras.Input`:


```python
# first model - title only
vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

vectorize_layer.adapt(train.map(lambda x, y: x["title"]))


title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)


```

Then we can start building the model. Since we are going to trian the models using two features, title and text, we need to use Keras Funcation API. The layers below are based on [this lecture](https://nbviewer.org/github/PhilChodrow/PIC16B/blob/master/lectures/tf/tf-4.ip).


```python
title_features = vectorize_layer(title_input)
title_features = layers.Embedding(size_vocabulary, 10, name = "embedding_title")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
title_output = layers.Dense(2, name = "fake")(title_features)

model1 = keras.Model(
    inputs = title_input,
    outputs = title_output
)

model1.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

title_history = model1.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                     verbose = False
                    )


```

We can write a helper function `history_plot` to visualize the training and validation accuracy:


```python
def history_plot(history1):
    plt.plot(history1.history["accuracy"], label = "training")
    plt.plot(history1.history["val_accuracy"], label = "validation")
    plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
    plt.legend()
    

```

As shown below, `model1` can achieve a validation accuary near 100% after 1 or 2 epoches: 


```python
history_plot(title_history)
```
    
![output_20_0.png](/images/output_20_0.png)
    

<br>
### Model 2 - text only

`model2` is built in the same way as `model1` except we adapt and train using the text data only:


```python
# second model - text only
vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

vectorize_layer.adapt(train.map(lambda x, y: x["text"]))



text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)

text_features = vectorize_layer(text_input)
text_features = layers.Embedding(size_vocabulary, 10, name = "embedding_text")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
text_output = layers.Dense(2, name = "fake")(text_features)

model2 = keras.Model(
    inputs = text_input,
    outputs = text_output
)

model2.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

text_history = model2.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = False
                    )

```
    
Similar to `model1`, `model2` can achieve a validation accuary near 100% after 1 epoch: 


```python
history_plot(text_history)
```
![output_24_0.png](/images/output_24_0.png)
    

<br>
### Model 3 - combine the previous 2 models
Now we can combine the two models using `layers.concatenate` and the model summary is shown below:


```python
# model 3 - both 
main = layers.concatenate([title_features, text_features], axis = 1)
output = layers.Dense(2, name = "fake")(main)


model3 = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)

model3.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)


model3.summary()
```

    Model: "model_2"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    title (InputLayer)              [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text (InputLayer)               [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text_vectorization (TextVectori (None, 500)          0           title[0][0]                      
    __________________________________________________________________________________________________
    text_vectorization_1 (TextVecto (None, 500)          0           text[0][0]                       
    __________________________________________________________________________________________________
    embedding_title (Embedding)     (None, 500, 10)      20000       text_vectorization[0][0]         
    __________________________________________________________________________________________________
    embedding_text (Embedding)      (None, 500, 10)      20000       text_vectorization_1[0][0]       
    __________________________________________________________________________________________________
    dropout (Dropout)               (None, 500, 10)      0           embedding_title[0][0]            
    __________________________________________________________________________________________________
    dropout_2 (Dropout)             (None, 500, 10)      0           embedding_text[0][0]             
    __________________________________________________________________________________________________
    global_average_pooling1d (Globa (None, 10)           0           dropout[0][0]                    
    __________________________________________________________________________________________________
    global_average_pooling1d_1 (Glo (None, 10)           0           dropout_2[0][0]                  
    __________________________________________________________________________________________________
    dropout_1 (Dropout)             (None, 10)           0           global_average_pooling1d[0][0]   
    __________________________________________________________________________________________________
    dropout_3 (Dropout)             (None, 10)           0           global_average_pooling1d_1[0][0] 
    __________________________________________________________________________________________________
    dense (Dense)                   (None, 32)           352         dropout_1[0][0]                  
    __________________________________________________________________________________________________
    dense_2 (Dense)                 (None, 32)           352         dropout_3[0][0]                  
    __________________________________________________________________________________________________
    dense_1 (Dense)                 (None, 32)           1056        dense[0][0]                      
    __________________________________________________________________________________________________
    dense_3 (Dense)                 (None, 32)           1056        dense_2[0][0]                    
    __________________________________________________________________________________________________
    concatenate (Concatenate)       (None, 64)           0           dense_1[0][0]                    
                                                                     dense_3[0][0]                    
    __________________________________________________________________________________________________
    fake (Dense)                    (None, 2)            130         concatenate[0][0]                
    ==================================================================================================
    Total params: 42,946
    Trainable params: 42,946
    Non-trainable params: 0
    __________________________________________________________________________________________________
    

Again the model can achieve near 100% accuracy:


```python
history = model3.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = False
                    )

history_plot(history)
```
![output_28_0.png](/images/output_28_0.png)
    

<br>
### Testing the model
We can test `model3` using the following test data, and we find the test accuracy is aboutp 99%:


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_df=pd.read_csv(test_url)
test_dataset = make_dataset(test_df)

loss, accuracy = model3.evaluate(test_dataset)
print('Test accuracy :', accuracy)
```

    225/225 [==============================] - 2s 10ms/step - loss: 0.0260 - accuracy: 0.9946
    Test accuracy : 0.994565486907959
    
<br>
### Visualize the embeddings 
The embeddings can be visualized using `PCA` to reduce the features to 2-dimensional weights:


```python
weights = model3.get_layer('embedding_text').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()     

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

# visualzing the text embedding
embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})

import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 5,
                 hover_name = "word")

fig.show()
```
{% include embedding_text1.html %}

```python
# visualzing the title embedding
weights_title = model3.get_layer('embedding_title').get_weights()[0]
weights_title = pca.fit_transform(weights_title)
embedding_title_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})

fig_title = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 5,
                 hover_name = "word")

fig_title.show()
```
{% include embedding_title1.html %}

The words that are far from the orgin suggests strong indication towards fake or true news. On the negative `x0` side we have "trumps" and "thats" while the "gop" and "21st" is on the postive `x0` side. Near the origin, we have "leaders". None of the words show strong indiction towards fake or true news. Therefore, fake news might be very hard to be identified by humans but easier detected by machine learning.
